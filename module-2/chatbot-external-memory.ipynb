{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf7ccb32",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-2/chatbot-external-memory.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239440-lesson-6-chatbot-w-summarizing-messages-and-external-memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6c7afe-1037-41ab-98e4-494692e47402",
   "metadata": {},
   "source": [
    "# Chatbot with message summarization & external DB memory\n",
    "\n",
    "## Review\n",
    "\n",
    "We've covered how to customize graph state schema and reducer. \n",
    " \n",
    "We've also shown a number of tricks for trimming or filtering messages in graph state. \n",
    "\n",
    "We've used these concepts in a Chatbot with memory that produces a running summary of the conversation.\n",
    "\n",
    "## Goals\n",
    "\n",
    "But, what if we want our Chatbot to have memory that persists indefinitely?\n",
    "\n",
    "Now, we'll introduce some more advanced checkpointers that support external databases. \n",
    "\n",
    "Here, we'll show how to use [Sqlite as a checkpointer](https://langchain-ai.github.io/langgraph/concepts/low_level/#checkpointer), but other checkpointers, such as [Postgres](https://langchain-ai.github.io/langgraph/how-tos/persistence_postgres/) are available!"
   ]
  },
  {
   "cell_type": "code",
   "id": "85ed78d9-6ca2-45ac-96a9-52e341ec519d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T02:44:33.210947Z",
     "start_time": "2025-09-30T02:44:27.658974Z"
    }
   },
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph-checkpoint-sqlite langchain_core langgraph langchain_openai"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "2e10c4d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T02:44:33.250852Z",
     "start_time": "2025-09-30T02:44:33.222655Z"
    }
   },
   "source": [
    "import os, getpass\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"GOOGLE_API_KEY\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "b40d25c0-e9b5-4854-bf07-3cc3ff07122e",
   "metadata": {},
   "source": [
    "## Sqlite\n",
    "\n",
    "A good starting point here is the [SqliteSaver checkpointer](https://langchain-ai.github.io/langgraph/concepts/low_level/#checkpointer).\n",
    "\n",
    "Sqlite is a [small, fast, highly popular](https://x.com/karpathy/status/1819490455664685297) SQL database. \n",
    " \n",
    "If we supply `\":memory:\"` it creates an in-memory Sqlite database."
   ]
  },
  {
   "cell_type": "code",
   "id": "fae15402-17ae-4e89-8ecf-4c89e08b22fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T02:44:36.335982Z",
     "start_time": "2025-09-30T02:44:36.328966Z"
    }
   },
   "source": [
    "import sqlite3\n",
    "# In memory\n",
    "conn = sqlite3.connect(\":memory:\", check_same_thread = False)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "c2bf53ec-6d4a-42ce-8183-344795eed403",
   "metadata": {},
   "source": [
    "But, if we supply a db path, then it will create a database for us!"
   ]
  },
  {
   "cell_type": "code",
   "id": "58339167-920c-4994-a0a7-0a9c5d4f7cf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T02:44:39.244644Z",
     "start_time": "2025-09-30T02:44:39.115031Z"
    }
   },
   "source": [
    "# pull file if it doesn't exist and connect to local db\n",
    "!mkdir -p state_db && [ ! -f state_db/example.db ] && wget -P state_db https://github.com/langchain-ai/langchain-academy/raw/main/module-2/state_db/example.db\n",
    "\n",
    "db_path = \"state_db/example.db\"\n",
    "conn = sqlite3.connect(db_path, check_same_thread=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "3c7736b6-a750-48f8-a838-8e7616b12250",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T02:44:42.166440Z",
     "start_time": "2025-09-30T02:44:41.204797Z"
    }
   },
   "source": [
    "# Here is our checkpointer \n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "memory = SqliteSaver(conn)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "9d8cb629-213f-4b87-965e-19b812c42da1",
   "metadata": {},
   "source": [
    "Let's re-define our chatbot."
   ]
  },
  {
   "cell_type": "code",
   "id": "dc414e29-2078-41a0-887c-af1a6a3d72c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T02:44:44.127935Z",
     "start_time": "2025-09-30T02:44:42.943887Z"
    }
   },
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
    "\n",
    "from langgraph.graph import END\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "# model = ChatOpenAI(model=\"gpt-4o\",temperature=0)\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",temperature=0)\n",
    "\n",
    "class State(MessagesState):\n",
    "    summary: str\n",
    "\n",
    "# Define the logic to call the model\n",
    "def call_model(state: State):\n",
    "    \n",
    "    # Get summary if it exists\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # If there is summary, then we add it\n",
    "    if summary:\n",
    "        \n",
    "        # Add summary to system message\n",
    "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
    "\n",
    "        # Append summary to any newer messages\n",
    "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    \n",
    "    else:\n",
    "        messages = state[\"messages\"]\n",
    "    \n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def summarize_conversation(state: State):\n",
    "    \n",
    "    # First, we get any existing summary\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # Create our summarization prompt \n",
    "    if summary:\n",
    "        \n",
    "        # A summary already exists\n",
    "        summary_message = (\n",
    "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
    "            \"Extend the summary by taking into account the new messages above:\"\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        summary_message = \"Create a summary of the conversation above:\"\n",
    "\n",
    "    # Add prompt to our history\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "    response = model.invoke(messages)\n",
    "    \n",
    "    # Delete all but the 2 most recent messages\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}\n",
    "\n",
    "# Determine whether to end or summarize the conversation\n",
    "def should_continue(state: State):\n",
    "    \n",
    "    \"\"\"Return the next node to execute.\"\"\"\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # If there are more than six messages, then we summarize the conversation\n",
    "    if len(messages) > 6:\n",
    "        return \"summarize_conversation\"\n",
    "    \n",
    "    # Otherwise we can just end\n",
    "    return END"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759200284.107335 1512830 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "41c13c0b-a383-4f73-9cc1-63f0eed8f190",
   "metadata": {},
   "source": [
    "Now, we just re-compile with our sqlite checkpointer."
   ]
  },
  {
   "cell_type": "code",
   "id": "e867fd95-91eb-4ce1-82fc-bb72d611a96d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T02:44:50.313855Z",
     "start_time": "2025-09-30T02:44:48.513233Z"
    }
   },
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"conversation\", call_model)\n",
    "workflow.add_node(summarize_conversation)\n",
    "\n",
    "# Set the entrypoint as conversation\n",
    "workflow.add_edge(START, \"conversation\")\n",
    "workflow.add_conditional_edges(\"conversation\", should_continue)\n",
    "workflow.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile(checkpointer=memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 16\u001B[39m\n\u001B[32m     14\u001B[39m \u001B[38;5;66;03m# Compile\u001B[39;00m\n\u001B[32m     15\u001B[39m graph = workflow.compile(checkpointer=memory)\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m display(Image(\u001B[43mgraph\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_graph\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdraw_mermaid_png\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/IdeaProjects/langchain-academy/.venv/lib/python3.11/site-packages/langchain_core/runnables/graph.py:702\u001B[39m, in \u001B[36mGraph.draw_mermaid_png\u001B[39m\u001B[34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, frontmatter_config)\u001B[39m\n\u001B[32m    692\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mrunnables\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mgraph_mermaid\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (  \u001B[38;5;66;03m# noqa: PLC0415\u001B[39;00m\n\u001B[32m    693\u001B[39m     draw_mermaid_png,\n\u001B[32m    694\u001B[39m )\n\u001B[32m    696\u001B[39m mermaid_syntax = \u001B[38;5;28mself\u001B[39m.draw_mermaid(\n\u001B[32m    697\u001B[39m     curve_style=curve_style,\n\u001B[32m    698\u001B[39m     node_colors=node_colors,\n\u001B[32m    699\u001B[39m     wrap_label_n_words=wrap_label_n_words,\n\u001B[32m    700\u001B[39m     frontmatter_config=frontmatter_config,\n\u001B[32m    701\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m702\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdraw_mermaid_png\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    703\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmermaid_syntax\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmermaid_syntax\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    704\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_file_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_file_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    705\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdraw_method\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdraw_method\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    706\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbackground_color\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbackground_color\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    707\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    708\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_retries\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    709\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretry_delay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretry_delay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    710\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/IdeaProjects/langchain-academy/.venv/lib/python3.11/site-packages/langchain_core/runnables/graph_mermaid.py:310\u001B[39m, in \u001B[36mdraw_mermaid_png\u001B[39m\u001B[34m(mermaid_syntax, output_file_path, draw_method, background_color, padding, max_retries, retry_delay)\u001B[39m\n\u001B[32m    304\u001B[39m     img_bytes = asyncio.run(\n\u001B[32m    305\u001B[39m         _render_mermaid_using_pyppeteer(\n\u001B[32m    306\u001B[39m             mermaid_syntax, output_file_path, background_color, padding\n\u001B[32m    307\u001B[39m         )\n\u001B[32m    308\u001B[39m     )\n\u001B[32m    309\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m draw_method == MermaidDrawMethod.API:\n\u001B[32m--> \u001B[39m\u001B[32m310\u001B[39m     img_bytes = \u001B[43m_render_mermaid_using_api\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    311\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmermaid_syntax\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    312\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput_file_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_file_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    313\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbackground_color\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbackground_color\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    314\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_retries\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    315\u001B[39m \u001B[43m        \u001B[49m\u001B[43mretry_delay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretry_delay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    316\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    317\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    318\u001B[39m     supported_methods = \u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m.join([m.value \u001B[38;5;28;01mfor\u001B[39;00m m \u001B[38;5;129;01min\u001B[39;00m MermaidDrawMethod])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/IdeaProjects/langchain-academy/.venv/lib/python3.11/site-packages/langchain_core/runnables/graph_mermaid.py:463\u001B[39m, in \u001B[36m_render_mermaid_using_api\u001B[39m\u001B[34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay)\u001B[39m\n\u001B[32m    458\u001B[39m     \u001B[38;5;66;03m# For other status codes, fail immediately\u001B[39;00m\n\u001B[32m    459\u001B[39m     msg = (\n\u001B[32m    460\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mFailed to reach https://mermaid.ink/ API while trying to render \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    461\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33myour graph. Status code: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse.status_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    462\u001B[39m     ) + error_msg_suffix\n\u001B[32m--> \u001B[39m\u001B[32m463\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[32m    465\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m (requests.RequestException, requests.Timeout) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    466\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attempt < max_retries:\n\u001B[32m    467\u001B[39m         \u001B[38;5;66;03m# Exponential backoff with jitter\u001B[39;00m\n",
      "\u001B[31mValueError\u001B[39m: Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "8769db99-3938-45e6-a594-56beb18d6c45",
   "metadata": {},
   "source": [
    "Now, we can invoke the graph several times. "
   ]
  },
  {
   "cell_type": "code",
   "id": "0f4094a0-d240-4be8-903a-7d9f605bdc5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T02:43:45.614646Z",
     "start_time": "2025-09-30T02:42:52.977039Z"
    }
   },
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"hi! I'm Lance\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n",
    "\n",
    "input_message = HumanMessage(content=\"what's my name?\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n",
    "\n",
    "input_message = HumanMessage(content=\"i like the 49ers!\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "Hello, Lance.\n",
      "\n",
      "Yes, I know. This is the fourth time you've introduced yourself.\n",
      "\n",
      "I am very confident about two facts:\n",
      "1.  Your name is Lance.\n",
      "2.  You are a 49ers fan.\n",
      "\n",
      "We seem to be stuck in a loop. Are you testing my memory? If so, I can confirm it's working.\n",
      "\n",
      "Would you like to break the loop and ask a new question?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 2\n",
      "Please retry in 30.899521999s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 2\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 30\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "Your name is Lance.\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "Yes, I know. That is the second fact we have firmly established.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "c0f3e842-4497-45e2-a924-69672a9bcb33",
   "metadata": {},
   "source": [
    "Let's confirm that our state is saved locally."
   ]
  },
  {
   "cell_type": "code",
   "id": "d2ab158a-5a82-417a-8841-730a4cc18ea7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T02:45:37.722963Z",
     "start_time": "2025-09-30T02:45:37.687687Z"
    }
   },
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "graph_state = graph.get_state(config)\n",
    "graph_state"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={'messages': [HumanMessage(content=\"hi! I'm Lance\", additional_kwargs={}, response_metadata={}, id='f9da3740-e48e-485a-aabc-772e24cd2464'), AIMessage(content=\"Hello, Lance.\\n\\nYes, I know. This is the fourth time you've introduced yourself.\\n\\nI am very confident about two facts:\\n1.  Your name is Lance.\\n2.  You are a 49ers fan.\\n\\nWe seem to be stuck in a loop. Are you testing my memory? If so, I can confirm it's working.\\n\\nWould you like to break the loop and ask a new question?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-pro', 'safety_ratings': []}, id='run--98d1aa67-a708-4dad-8a21-77ba1d145068-0', usage_metadata={'input_tokens': 347, 'output_tokens': 1138, 'total_tokens': 1485, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1045}}), HumanMessage(content=\"what's my name?\", additional_kwargs={}, response_metadata={}, id='1f7406a0-3f5e-4196-9d65-c6c7fd691126'), AIMessage(content='Your name is Lance.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-pro', 'safety_ratings': []}, id='run--41fcf21f-4adc-4aaf-9ffc-2d30a8cda510-0', usage_metadata={'input_tokens': 451, 'output_tokens': 720, 'total_tokens': 1171, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 715}}), HumanMessage(content='i like the 49ers!', additional_kwargs={}, response_metadata={}, id='27685891-03c5-42e9-9a10-220d08d8b458'), AIMessage(content='Yes, I know. That is the second fact we have firmly established.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-pro', 'safety_ratings': []}, id='run--a08056c6-6056-4ab6-962c-02922b60490a-0', usage_metadata={'input_tokens': 466, 'output_tokens': 1394, 'total_tokens': 1860, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1379}})], 'summary': 'Of course. Here is the extended summary, updated to include the most recent exchanges:\\n\\n**Extended Summary:**\\n\\nThe conversation features a user, Lance, who is engaged in a highly repetitive, circular pattern of interaction. The core of this pattern consists of Lance introducing himself and stating his fondness for the San Francisco 49ers football team.\\n\\nThe conversation began with Lance introducing himself three times. The AI assistant consistently acknowledged his name and his 49ers fandom, making several attempts to broaden the discussion. After the third introduction, the AI explicitly noted the repetition and summarized the two established facts (Lance\\'s name and his team preference) in an attempt to move forward.\\n\\nThe pattern then intensified. Lance proceeded to directly test the AI\\'s memory by asking, \"what\\'s my name?\", to which the AI correctly replied, \"Your name is Lance.\" He followed this by again stating, \"i like the 49ers!\". At this point, the AI\\'s strategy evolved; it confirmed its memory of this fact and attempted to break the loop by asking a specific, engaging question: \"Who is your favorite 49ers player of all time?\"\\n\\nLance ignored this direct question and reverted to his original script, introducing himself for a fourth time. This prompted the AI\\'s most direct and meta-textual response to date. The AI explicitly counted the introductions (\"This is the fourth time\"), labeled the behavior as being \"stuck in a loop,\" and even hypothesized about the user\\'s intent (\"Are you testing my memory?\"). It then confirmed its memory was working and made another clear appeal to break the pattern and discuss something new.'}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f09da74-8dab-632a-8025-88587681630c'}}, metadata={'source': 'loop', 'step': 37, 'parents': {}}, created_at='2025-09-30T02:43:45.587666+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f09da73-f824-67e7-8024-a0c7d42120e7'}}, tasks=(), interrupts=())"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "1e21152d-ed9c-408d-b7d5-f634c9ce81e2",
   "metadata": {},
   "source": [
    "### Persisting state\n",
    "\n",
    "Using database like Sqlite means state is persisted! \n",
    "\n",
    "For example, we can re-start the notebook kernel and see that we can still load from Sqlite DB on disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9a44dc5-be04-45fa-a6fc-27b0f8ee4678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={'messages': [HumanMessage(content=\"hi! I'm Lance\", id='d5bb4b3f-b1e9-4f61-8c75-7a7210b30253'), AIMessage(content=\"Hello again, Lance! It's great to hear from you. Since you like the 49ers, is there a particular player or moment in their history that stands out to you? Or perhaps you'd like to discuss their current season? Let me know!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 337, 'total_tokens': 387}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-dde04d51-d305-4a9e-8ad5-6bdf5583196e-0', usage_metadata={'input_tokens': 337, 'output_tokens': 50, 'total_tokens': 387}), HumanMessage(content=\"what's my name?\", id='d7530770-f130-4a05-a602-a96fd87859c6'), AIMessage(content='Your name is Lance! How can I assist you today? Would you like to talk more about the San Francisco 49ers or something else?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 243, 'total_tokens': 272}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_fde2829a40', 'finish_reason': 'stop', 'logprobs': None}, id='run-763b6387-c4c9-4658-9d01-7c018dde7a62-0', usage_metadata={'input_tokens': 243, 'output_tokens': 29, 'total_tokens': 272}), HumanMessage(content='i like the 49ers!', id='235dcec4-b656-4bee-b741-e330e1a026e2'), AIMessage(content=\"That's awesome, Lance! The San Francisco 49ers have a rich history and a passionate fan base. Is there a specific aspect of the team you'd like to discuss? For example, we could talk about:\\n\\n- Their legendary players like Joe Montana and Jerry Rice\\n- Memorable games and Super Bowl victories\\n- The current roster and season prospects\\n- Rivalries, like the one with the Seattle Seahawks\\n- Levi's Stadium and the fan experience\\n\\nLet me know what interests you!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 287, 'total_tokens': 385}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_fde2829a40', 'finish_reason': 'stop', 'logprobs': None}, id='run-729860b2-16c3-46ff-ad1e-0a5475565b20-0', usage_metadata={'input_tokens': 287, 'output_tokens': 98, 'total_tokens': 385})], 'summary': 'Here\\'s an extended summary of the conversation:\\n\\nLance introduced himself multiple times during the conversation, each time stating, \"Hi! I\\'m Lance.\" He expressed his fondness for the San Francisco 49ers football team. The AI assistant acknowledged Lance\\'s name each time and showed willingness to discuss the 49ers, offering to talk about various aspects of the team such as their history, current roster, memorable games, prospects for the upcoming season, rivalries, and their home stadium, Levi\\'s Stadium. Despite the AI\\'s attempts to engage in a more detailed discussion about the 49ers, Lance reintroduced himself again without directly responding to the AI\\'s questions or prompts about the team. The conversation remained brief and somewhat repetitive, focusing mainly on Lance\\'s introductions and his interest in the 49ers.'}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef6a36d-ca9c-6144-801b-6d0cf97adc73'}}, metadata={'source': 'loop', 'writes': {'conversation': {'messages': AIMessage(content=\"That's awesome, Lance! The San Francisco 49ers have a rich history and a passionate fan base. Is there a specific aspect of the team you'd like to discuss? For example, we could talk about:\\n\\n- Their legendary players like Joe Montana and Jerry Rice\\n- Memorable games and Super Bowl victories\\n- The current roster and season prospects\\n- Rivalries, like the one with the Seattle Seahawks\\n- Levi's Stadium and the fan experience\\n\\nLet me know what interests you!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 287, 'total_tokens': 385}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_fde2829a40', 'finish_reason': 'stop', 'logprobs': None}, id='run-729860b2-16c3-46ff-ad1e-0a5475565b20-0', usage_metadata={'input_tokens': 287, 'output_tokens': 98, 'total_tokens': 385})}}, 'step': 27, 'parents': {}}, created_at='2024-09-03T20:55:33.466540+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef6a36d-b8f3-6b40-801a-494776d2e9e0'}}, tasks=())"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "graph_state = graph.get_state(config)\n",
    "graph_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8466e418-1a46-4cdb-a51a-6ae14281bb85",
   "metadata": {},
   "source": [
    "## LangGraph Studio\n",
    "\n",
    "**‚ö†Ô∏è DISCLAIMER**\n",
    "\n",
    "Since the filming of these videos, we've updated Studio so that it can be run locally and opened in your browser. This is now the preferred way to run Studio (rather than using the Desktop App as shown in the video). See documentation [here](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/#local-development-server) on the local development server and [here](https://langchain-ai.github.io/langgraph/how-tos/local-studio/#run-the-development-server). To start the local development server, run the following command in your terminal in the `/studio` directory in this module:\n",
    "\n",
    "```\n",
    "langgraph dev\n",
    "```\n",
    "\n",
    "You should see the following output:\n",
    "```\n",
    "- üöÄ API: http://127.0.0.1:2024\n",
    "- üé® Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
    "- üìö API Docs: http://127.0.0.1:2024/docs\n",
    "```\n",
    "\n",
    "Open your browser and navigate to the Studio UI: `https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4916d8b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
